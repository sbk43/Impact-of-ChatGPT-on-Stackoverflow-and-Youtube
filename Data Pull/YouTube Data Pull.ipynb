{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "* This python notebook is developed to pull the Youtube data using [Youtube API](https://developers.google.com/youtube/v3/docs) which is publicly available.\n",
        "\n",
        "* YouTube Channels are selected for analysis and pulled the list of videos with video_id, description, title, published date, view count, like count and comment count.\n",
        "\n",
        "* Comments are pulled for each video along with date of posted and number of likes received for the respective comment.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "obzUyyXgyQFp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import required dependencies"
      ],
      "metadata": {
        "id": "G2jpChUQyNJa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "pF8LP4KjRQ5V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "API Key to authorize and pull the data this is generated from the Google account and Cloud.\n",
        "\n",
        "**Caveat:** API exhausts for a API Key as quota exceed per day. Refer [documentation](https://developers.google.com/youtube/v3/guides/quota_and_compliance_audits) for more information.\n",
        "\n",
        "\n",
        "For creation of youtube api key, one has to have a goole account and should signed into Gloud Cloud Platform (GCP) and enable YouTube API key from services. Refer the [youtube video](https://colab.research.google.com/drive/1mvJXAdRaSCPMUcuqGCjb97lBMQhwRgrD?usp=sharing) to create the api_key.\n",
        "\n"
      ],
      "metadata": {
        "id": "9okNJjZb0_kt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "api_key = '<place your api key>'"
      ],
      "metadata": {
        "id": "DhyvajLpRS2y"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "List of channels for research. Use this [link](https://urldefense.com/v3/__https:/www.streamweasels.com/tools/youtube-channel-id-and-user-id-convertor/__;!!DZ3fjg!9vMbpkNVZM8aKwDI4WcGxRIkT9XnF2xVtl8OTuT6_0tVMUpHJKyI_fm-kKM8uDC57ogvbSpGeMbNqAyyHgid1Q$) to get the channel_id for the respective YouTube Channel."
      ],
      "metadata": {
        "id": "Ws-lfrx81F1x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "channels = {\n",
        "    'DeepLearning.AI': 'UCcIXc5mJsHVYTZR1maL5l9w',\n",
        "    'StatQuest': 'UCtYLUTtgS3k1Fg4y5tAhLbw',\n",
        "    'Sentdex': 'UCfzlCWGWYyIQ0aLC5w48gBQ',\n",
        "    '3Blue1Brown': 'UCYO_jab_esuFRV4b17AJtAw',\n",
        "    'Siraj Raval': 'UCWN3xxRkmTPmbKwht9FuE5A',\n",
        "    'Two Minute Papers': 'UCbfYPyITQ-7l4upoX8nvctg',\n",
        "    'Lexfridman': 'UCSHZKyawb77ixDdsGog4iWA'\n",
        "}"
      ],
      "metadata": {
        "id": "n5ThthZTRVdB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Defined a function to pull the list of videos for each channel. Use the endpoint https://www.googleapis.com/youtube/v3/search"
      ],
      "metadata": {
        "id": "hoOI9rf-18k9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G4bLA9A5OBlw"
      },
      "outputs": [],
      "source": [
        "def get_channel_videos(api_key, channel_id):\n",
        "    videos_base_url = \"https://www.googleapis.com/youtube/v3/search\"\n",
        "    next_page_token = None\n",
        "    all_videos = []\n",
        "\n",
        "    while True:\n",
        "        videos_params = {\n",
        "            'key': api_key,\n",
        "            'channelId': channel_id,\n",
        "            'part': 'snippet',\n",
        "            'order': 'date',\n",
        "            'maxResults': 50,\n",
        "            'type': 'video',\n",
        "            'pageToken': next_page_token\n",
        "        }\n",
        "\n",
        "        videos_response = requests.get(videos_base_url, params=videos_params)\n",
        "        videos_data = videos_response.json()\n",
        "        video_ids = [item['id']['videoId'] for item in videos_data['items']]\n",
        "\n",
        "        # Fetch details for the current batch of video IDs\n",
        "        if video_ids:\n",
        "            details_base_url = \"https://www.googleapis.com/youtube/v3/videos\"\n",
        "            details_params = {\n",
        "                'key': api_key,\n",
        "                'id': ','.join(video_ids),\n",
        "                'part': 'snippet,statistics'\n",
        "            }\n",
        "\n",
        "            details_response = requests.get(details_base_url, params=details_params)\n",
        "            details_data = details_response.json()\n",
        "\n",
        "            for item in details_data['items']:\n",
        "                all_videos.append({\n",
        "                    'channel_id': item['snippet']['channelId'],\n",
        "                    'channel_name': item['snippet']['channelTitle'],\n",
        "                    'video_id': item['id'],\n",
        "                    'title': item['snippet']['title'],\n",
        "                    'description': item['snippet']['description'],\n",
        "                    'published_at': item['snippet']['publishedAt'],\n",
        "                    'view_count': item['statistics'].get('viewCount', '0'),\n",
        "                    'like_count': item['statistics'].get('likeCount', '0'),\n",
        "                    'comment_count': item['statistics'].get('commentCount', '0')\n",
        "                })\n",
        "\n",
        "        next_page_token = videos_data.get('nextPageToken')\n",
        "        if not next_page_token:\n",
        "            break\n",
        "\n",
        "    return all_videos\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fetch the data (list of videos) using the loop and store in the csv file - youtube_channel_videos_details."
      ],
      "metadata": {
        "id": "fhhuBqoJ2B83"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_videos = []\n",
        "for channel_name, channel_id in channels.items():\n",
        "    print(f\"Fetching videos for channel: {channel_name}\")\n",
        "    videos = get_channel_videos(api_key, channel_id)\n",
        "    all_videos.extend(videos)\n",
        "\n",
        "# Convert list of videos to DataFrame\n",
        "videos_df = pd.DataFrame(all_videos)\n",
        "\n",
        "# Optionally, save to CSV\n",
        "videos_df.to_csv('youtube_channel_videos_details.csv', index=False)\n",
        "print(\"Data extraction complete. Check the CSV file.\")"
      ],
      "metadata": {
        "id": "ncEaCZXFtaM2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "70705d67-5c41-49b5-e1b8-01ef6a3aca51"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetching videos for channel: DeepLearning.AI\n",
            "Fetching videos for channel: StatQuest\n",
            "Fetching videos for channel: Sentdex\n",
            "Fetching videos for channel: 3Blue1Brown\n",
            "Fetching videos for channel: Siraj Raval\n",
            "Fetching videos for channel: Two Minute Papers\n",
            "Fetching videos for channel: Lexfridman\n",
            "Data extraction complete. Check the CSV file.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Function definition for data pull of comments for each video. Use the end point https://www.googleapis.com/youtube/v3/commentThreads"
      ],
      "metadata": {
        "id": "syS1ICwI3DkA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_video_comments(api_key, video_id):\n",
        "    comments_base_url = \"https://www.googleapis.com/youtube/v3/commentThreads\"\n",
        "    comments_params = {\n",
        "        'key': api_key,\n",
        "        'videoId': video_id,\n",
        "        'part': 'snippet',\n",
        "        'maxResults': 100,\n",
        "        'textFormat': 'plainText'\n",
        "    }\n",
        "    all_comments = []\n",
        "    next_page_token = None\n",
        "\n",
        "    while True:\n",
        "        if next_page_token:\n",
        "            comments_params['pageToken'] = next_page_token\n",
        "\n",
        "        response = requests.get(comments_base_url, params=comments_params)\n",
        "        if response.status_code == 403:\n",
        "            print(\"403 - Access Forbidden. API key or quota may be exhausted.\")\n",
        "            return all_comments, False  # Return collected comments and False indicating failure due to API limits.\n",
        "\n",
        "        data = response.json()\n",
        "\n",
        "        for item in data['items']:\n",
        "            top_level_comment = item['snippet']['topLevelComment']\n",
        "            top_level_comment_snippet = top_level_comment['snippet']\n",
        "            all_comments.append({\n",
        "                'video_id': video_id,\n",
        "                'comment_id': top_level_comment['id'],\n",
        "                'text': top_level_comment_snippet['textDisplay'],\n",
        "                'author': top_level_comment_snippet['authorDisplayName'],\n",
        "                'like_count': top_level_comment_snippet['likeCount'],\n",
        "                'published_at': top_level_comment_snippet['publishedAt']\n",
        "            })\n",
        "\n",
        "        next_page_token = data.get('nextPageToken')\n",
        "        if not next_page_token:\n",
        "            break\n",
        "\n",
        "    return all_comments, True"
      ],
      "metadata": {
        "id": "EzDOEltCKFsv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Presetting to pull the data and see where the API exhausts"
      ],
      "metadata": {
        "id": "0wmjFEXU3MgD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_comments = []\n",
        "successful = True\n",
        "last_index = 0"
      ],
      "metadata": {
        "id": "cNAKQbHBiYwt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Execute the above function for pulling the comments data, this loop stops when the quota exceeds/API exhausts. And with minimum effort in adjusting the index from where to start, the rest of the data can be pulled on next day."
      ],
      "metadata": {
        "id": "qLHcgrlR3Zfu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for index, video_id in enumerate(videos_df['video_id']):\n",
        "    print(f\"Fetching comments for video ID: {video_id}\")\n",
        "    video_comments, successful = get_video_comments(api_key, video_id)\n",
        "    all_comments.extend(video_comments)\n",
        "    if not successful:\n",
        "        last_index = index\n",
        "        break\n",
        "\n",
        "# Convert list of comments to DataFrame\n",
        "comments_df = pd.DataFrame(all_comments)\n",
        "# Optionally, save to CSV\n",
        "comments_df.to_csv('youtube_video_comments.csv', index=False)\n",
        "print(f\"Comments data extraction stopped at index {last_index}. Check the CSV file for data.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yp1Jy9x9KGN-",
        "outputId": "8291ab25-96d9-4c25-e2ad-214aaca43bab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetching comments for video ID: kk6rh45r0fo\n",
            "403 - Access Forbidden. API key or quota may be exhausted.\n",
            "Comments data extraction stopped at index 0. Check the CSV file for data.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Comments data is stored to csv - youtube_video_comments"
      ],
      "metadata": {
        "id": "I9J2RkGF4Wil"
      }
    }
  ]
}